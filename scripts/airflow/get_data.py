import requests
import json
import time
import pandas as pd
import datetime as dt
import pickle
from typing import List, Any, Dict


class ExtendedDict(dict):
    def batch_add(self, batch: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å–ø–∏—Å–∫–æ–≤ –ø–æ –∫–ª—é—á–∞–º - –∫–∞–∂–¥–æ–º—É —Å–ø–∏—Å–∫—É –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é append.
        –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ pd.concat, –Ω–æ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö dict.

        –û–±–Ω–æ–≤–ª—è–µ–º—ã–µ –∫–ª—é—á–∏ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–ª–æ—Å–∫–∏–µ —Å–ø–∏—Å–∫–∏!

        Args:
            target_dict (Dict[str, List[Any]]):
            batch (Dict[str, Any]): –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Ü–µ–ª–µ–≤–æ–π —Å–ª–æ–≤–∞—Ä—å
        """
        for k, v in batch.items():
            self[k].append(v)


class VacanciesDumper:
    """–ö–ª–∞—Å—Å –¥–ª—è –¥–∞–º–ø–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å hh.ru"""

    def __init__(
        self, url: str, search_queries: List[str], area: str, prev_data: pd.DataFrame = None, timeout: int = 2
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞

        Args:
            url (str): —ç–Ω–¥–ø–æ–∏–Ω—Ç api, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–π.
            search_queries (List[str]): —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ (—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö).
            area (str): id —Ä–µ–≥–∏–æ–Ω–∞ (—Å–º. —Ç–∞–±–ª–∏—á–∫—É `areas`).
            prev_data (pd.DataFrame, Optional): –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ, —Ç–æ –æ–±–æ–≥–æ—â–∞–µ–º –∏—Ö, —Ç—É—Ç –∂–µ MLOps,
                –ø–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å, –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ, –≤—Å–µ –¥–µ–ª–∞. Defaults to None.
            timeout (int): –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø–∞—É–∑–∞ –≤ <timeout> –º—Å, —á—Ç–æ –± –Ω–µ –±–∞–Ω–∏–ª–∏. Defaults to 2.

        Returns:
            pd.DataFrame: —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª—è:
                id - –µ—Å–ª–∏ –¥–æ –ø—Ä–æ–¥–∞ –¥–æ–π–¥–µ—Ç - –æ—Ç–¥–∞–≤–∞—Ç—å –∫–∞—Ä—Ç–æ—á–∫–∏ —Ä–∞–±–æ—Ç—ãüòê [id]
                name - –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ [vac_name]
                salary - –∑–∞—Ä–ø–ª–∞—Ç–∞:
                    from - –æ—Ç [salary_from]
                    to - –¥–æ [salary_to]
                published_at - —Ä–∞–∑–º–µ—â–µ–Ω–∞ [datetime]
                type [id] - –µ—Å–ª–∏ –æ—Ç–∫—Ä—Ç–∞ (–æ—Ü–µ–Ω–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ–∏—Å–∫–∞ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞) [state]
                employer - —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å:
                    id - id —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—è (–ø–æ—Ç–æ–º –º–æ–∂–Ω–æ –¥–µ—Ä–Ω—É—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏—Ö —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π) [employer_id]
                    name - –∏–º—è —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—è [employer_name]
                snippet - –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    requirement - —á—Ç–æ –Ω—É–∂–Ω–æ —Å–æ–∏—Å–∫–∞—Ç–µ–ª—é [requirement]
                    responsibility - —á—Ç–æ –±—É–¥–µ—Ç –¥–µ–ª–∞—Ç—å [responsibility]
                schedule [id] - —Ç–∏–ø —Ä–∞–±–æ—Ç—ã (–æ—Ñ–∏—Å, —É–¥–∞–ª–µ–Ω–∫–∞ –∏ —Ç.–¥.) [schedule]
                experience [name] - —Ç—Ä–µ–±—É–µ–º—ã–π –æ–ø—ã—Ç [experience]
                employment [id] - —Ç–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ [employment]
                professional_roles - —Ä–æ–ª–∏ –Ω–∞ —Ä–∞–±–æ—Ç–µ (–æ–ø—è—Ç—å –∂–µ, –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è) - —Ö—Ä–∞–Ω–∏—Ç—å –±—É–¥–µ–º –ø—Ä—è–º —Å—Ç—Ä–æ–∫–æ–π [roles]
        """

        self.page = 0
        self.pages = None
        if prev_data is None:
            prev_data = pd.DataFrame(self.get_template())
        self.data = prev_data
        self.url = url
        self.area = area
        self.search_queries = search_queries
        self.timeout = timeout

        self.new_vacancies = 0
        self.drop_vacancies = 0

    @staticmethod
    def get_template() -> Dict[str, List[str]]:
        return ExtendedDict(
            {
                "id": [],
                "vac_name": [],
                "salary_from": [],
                "salary_to": [],
                "datetime": [],
                "state": [],
                "employer_id": [],
                "employer_name": [],
                "requirement": [],
                "responsibility": [],
                "schedule": [],
                "experience": [],
                "employment": [],
                "roles": [],
            }
        )

    @staticmethod
    def __log(message) -> None:
        ts = dt.datetime.now().strftime("%d.%m.%y %H:%M:%S")
        print(f"[{ts}] {message}")

    def __dump_page(self, query) -> None:
        """–î–∞–º–ø –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""

        with requests.get(self.url, {"area": self.area, "per_page": 100, "text": query, "page": self.page}) as req:
            raw_data = json.loads(req.content.decode())

        _temp_dict = self.get_template()
        for vac in raw_data["items"]:
            if vac["id"] in self.data["id"]:
                self.drop_vacancies += 1
            else:
                salary = {} if vac["salary"] is None else vac["salary"]
                employer = {} if vac["employer"] is None else vac["employer"]
                snippet = {} if vac["snippet"] is None else vac["snippet"]
                schedule = {} if vac["schedule"] is None else vac["schedule"]
                state = {} if vac["type"] is None else vac["type"]
                experience = {} if vac["experience"] is None else vac["experience"]
                employment = {} if vac["employment"] is None else vac["employment"]

                _temp_dict.batch_add(
                    {
                        "id": vac["id"],
                        "vac_name": vac["name"],
                        "salary_from": salary.get("from", None),
                        "salary_to": salary.get("from", None),
                        "datetime": vac["published_at"],
                        "state": state.get("id", None),
                        "employer_id": employer.get("id", None),
                        "employer_name": employer.get("id", None),
                        "requirement": snippet.get("requirement", None),
                        "responsibility": snippet.get("responsibility", None),
                        "schedule": schedule.get("id", None),
                        "experience": experience.get("name", None),
                        "employment": employment.get("id", None),
                        "roles": vac["professional_roles"],
                    }
                )
                self.new_vacancies += 1

        self.data = pd.concat([self.data, pd.DataFrame(_temp_dict)], axis=0, ignore_index=True)

        self.page += 1
        if self.pages is None:
            self.pages = int(raw_data["pages"])

    def __dump_query(self, query) -> None:
        """–î–∞–º–ø –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ `search_queries`"""

        self.page = 0
        self.__dump_page(query)
        time.sleep(self.timeout)

        while self.page < self.pages:
            self.__dump_page(query)
            self.__log(f'–ó–∞–ø—Ä–æ—Å "{query}", —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {self.page}/{self.pages}')
            time.sleep(self.timeout)

    def dump(self) -> pd.DataFrame:
        """–ù–∞—á–∞–ª–æ –¥–∞–º–ø–∞ –≤–∞–∫–∞–Ω—Å–∏–π"""

        self.__log(f"–ù–∞—á–∏–Ω–∞—é –¥–∞–º–ø {len(self.search_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        for query in self.search_queries:
            self.__dump_query(query)
            self.pages = None
            self.__log(f'–ó–∞–ø—Ä–æ—Å "{query}" –æ–±—Ä–∞–±–æ—Ç–∞–Ω!')

        self.__log(f"–î–∞–º–ø –∑–∞–≤–µ—Ä—à–µ–Ω! –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π: {self.new_vacancies}, –æ—Ç–±—Ä–æ—à–µ–Ω–æ: {self.drop_vacancies}")
        return self.data


if __name__ == "__main__":
    dumper = VacanciesDumper(
        "https://api.hh.ru/vacancies",
        search_queries=["it", "ml engineer", "data engineer", "computer vision", "nlp", "neural networks"],
        area="1",
    )

    hh_data = dumper.dump()
    pickle.dump(hh_data, open("/scripts/airflow/data/hh_data", "wb"))
